{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision.datasets import FashionMNIST, MNIST, CIFAR10, SVHN\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.utils as vision_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "DEVICE = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_to_device(dataset,device=None):\n",
    "    final_X, final_Y = [], []\n",
    "    for x, y in dataset:\n",
    "        final_X.append(x)\n",
    "        final_Y.append(y)\n",
    "    X = torch.stack(final_X)\n",
    "    Y = torch.tensor(final_Y)\n",
    "    if device is not None:\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "    return torch.utils.data.TensorDataset(X, Y)\n",
    "\n",
    "\n",
    "def get_mnist_dl(batch_size_train=256, batch_size_eval=256, device=torch.device('cpu')):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "    data_train = MNIST('./datasets', train=True, download=True, transform=transform)\n",
    "    data_train = switch_to_device(data_train, device=device)\n",
    "    data_train, data_valid = torch.utils.data.random_split(data_train, [55000,5000])\n",
    "    \n",
    "    data_test = MNIST('./datasets', train=False, download=True, transform=transform)\n",
    "    data_test = switch_to_device(data_test, device=device)\n",
    "    \n",
    "    train_dl = DataLoader(data_train, batch_size=batch_size_train, shuffle=True)\n",
    "    valid_dl = DataLoader(data_valid, batch_size=batch_size_eval, shuffle=False)\n",
    "    test_dl = DataLoader(data_test, batch_size=batch_size_eval, shuffle=False)\n",
    "    \n",
    "    return train_dl, valid_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP_Net(nn.Module):\n",
    "\n",
    "  def __init__(self, num_classes=10) -> None:\n",
    "    super().__init__()\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.fc1 = nn.Linear(28*28, 1024)\n",
    "    self.Relu1 = nn.ReLU()\n",
    "    self.fc2 = nn.Linear(1024, 1024)\n",
    "    self.Relu2 = nn.ReLU()\n",
    "    self.fc3 = nn.Linear(1024, num_classes)\n",
    "    #self.softmax = nn.Softmax()\n",
    "\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    x = self.flatten(x)\n",
    "    x = self.Relu1(self.fc1(x))\n",
    "    x = self.Relu2(self.fc2(x))\n",
    "    x = self.fc3(x)\n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Net(nn.Module):\n",
    "    def __init__(self, num_classes=10) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 26, kernel_size=5, stride=1, padding = 0)\n",
    "        self.maxpooling1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(26, 52, kernel_size=3, stride=1, padding = 0)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(52, 10, kernel_size=1, stride=1, padding=0)\n",
    "        self.maxpooling3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc_1 = nn.Linear(5*5*10, 1000)\n",
    "        self.fc_2 = nn.Linear(1000, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.maxpooling1(F.relu(self.conv1(x)))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxpooling3(F.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc_1(x))\n",
    "        x = self.fc_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(stats):\n",
    "\n",
    "  fig, (ax1, ax2) = plt.subplots(1,2,figsize=(7,3), dpi=110)\n",
    "  ax1.grid()\n",
    "  ax2.grid()\n",
    "\n",
    "  ax1.set_title(\"ERM loss\")\n",
    "  ax2.set_title(\"Valid Acc\")\n",
    "  \n",
    "  ax1.set_xlabel(\"iterations\")\n",
    "  ax2.set_xlabel(\"iterations\")\n",
    "\n",
    "  itrs = [x[0] for x in stats['train-loss']]\n",
    "  loss = [x[1] for x in stats['train-loss']]\n",
    "  ax1.plot(itrs, loss)\n",
    "\n",
    "  itrs = [x[0] for x in stats['valid-acc']]\n",
    "  acc = [x[1] for x in stats['valid-acc']]\n",
    "  ax2.plot(itrs, acc)\n",
    "\n",
    "  ax1.set_ylim(0.0, 4.05)\n",
    "  ax2.set_ylim(0.0, 1.05)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_acc(model, dl):\n",
    "  model.eval()\n",
    "  acc = []\n",
    "  for X, y in dl:\n",
    "    #acc.append((torch.sigmoid(model(X)) > 0.5) == y)\n",
    "    acc.append(torch.argmax(model(X), dim=1) == y)\n",
    "  acc = torch.cat(acc)\n",
    "  acc = torch.sum(acc)/len(acc)\n",
    "  model.train()\n",
    "  return acc.item()\n",
    "\n",
    "\n",
    "def run_experiment(model, opt, train_dl, valid_dl, test_dl, use_forward_grad=False, num_forward_grad=1, max_epochs=20):\n",
    "\n",
    "  itr = -1\n",
    "  stats = {'train-loss': [], 'valid-acc':[]}\n",
    "  time_list = []\n",
    "\n",
    "  for epoch in range(max_epochs):\n",
    "    for x, y in train_dl:\n",
    "        itr += 1\n",
    "        layer_inputs = {}\n",
    "        def hook(mod, input):\n",
    "            layer_inputs[mod] = input[0]\n",
    "        for module in model.modules():\n",
    "            module.register_forward_pre_hook(hook)\n",
    "        opt.zero_grad()\n",
    "        start_2 = time.clock()\n",
    "        print(torch.cuda.memory_allocated()/1024/1024)\n",
    "        loss = F.cross_entropy(model(x), y)\n",
    "        print(torch.cuda.memory_allocated()/1024/1024)\n",
    "        loss.backward()\n",
    "        print(torch.cuda.memory_allocated()/1024/1024)\n",
    "        \n",
    "        if use_forward_grad:\n",
    "          with torch.no_grad():\n",
    "            #v_list = []\n",
    "            da = torch.zeros(num_forward_grad, 1).to(DEVICE)\n",
    "            for p in model.parameters():\n",
    "              g = p.grad.view(-1)\n",
    "              v = torch.randn(num_forward_grad, len(g), device=DEVICE)\n",
    "              da  = da + (v @ g).view(num_forward_grad,1)\n",
    "              #v_list.append(v)\n",
    "              g = ((v @ g).view(num_forward_grad,1) * v).mean(dim=0)\n",
    "              p.grad = g.view(p.grad.shape)\n",
    "        '''\n",
    "        if use_forward_grad:\n",
    "          with torch.no_grad():\n",
    "            #v_list = []\n",
    "            da = torch.zeros(num_forward_grad, 1).to(DEVICE)\n",
    "            for p in model.parameters():\n",
    "              g = p.grad.view(-1)\n",
    "              v = torch.randn(num_forward_grad, len(g), device=DEVICE)\n",
    "              source = torch.randn(len(g), device=DEVICE)\n",
    "              gap = math.floor(len(g)/num_forward_grad)\n",
    "              for i in range(num_forward_grad):\n",
    "                if i<num_forward_grad-1:\n",
    "                  v[i] = F.pad(source[i*gap:(i+1)*gap], pad = (i*gap, len(g)-(i+1)*gap))\n",
    "                else:\n",
    "                  v[i] = F.pad(source[i*gap:], pad = (i*gap, 0))\n",
    "              da  = da + (v @ g).view(num_forward_grad,1)\n",
    "              #v_list.append(v)\n",
    "              g = ((v @ g).view(num_forward_grad,1) * v).mean(dim=0)\n",
    "              p.grad = g.view(p.grad.shape)\n",
    " \n",
    "        \n",
    "        if use_forward_grad:\n",
    "          with torch.no_grad():\n",
    "            for name, module in model.named_modules():\n",
    "              if isinstance(module, torch.nn.Linear):\n",
    "                grad_w = module.weight.grad\n",
    "                approx = using_inputs_project(grad_w, layer_inputs[module])\n",
    "                module.weight.grad = approx\n",
    "                grad_b = module.bias.grad.view(-1)\n",
    "                v = torch.randn(num_forward_grad, len(grad_b), device=DEVICE)\n",
    "                grad_b = ((v @ grad_b).view(num_forward_grad,1) * v).mean(dim=0)\n",
    "                module.bias.grad = grad_b.view(module.bias.grad.shape)\n",
    "        '''\n",
    "        \n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.25, norm_type=2)\n",
    "        opt.step()\n",
    "        print(torch.cuda.memory_allocated()/1024/1024)\n",
    "        time_list.append(time.clock()-start_2)\n",
    "\n",
    "        stats['train-loss'].append((itr, loss.item()))\n",
    "\n",
    "        if itr % 20 == 0:\n",
    "          valid_acc = get_acc(model, valid_dl)\n",
    "          stats['valid-acc'].append((itr, valid_acc))\n",
    "          s = f\"{epoch}:{itr} [train] loss:{loss.item():.3f}, [valid] acc:{valid_acc:.3f}, time: {np.sum(time_list)/len(time_list)}\"\n",
    "          print(s)\n",
    "          time_list = []\n",
    "\n",
    "  test_acc = get_acc(model, test_dl)\n",
    "  print(f\"[test] acc:{test_acc:.3f}\")\n",
    "  return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def using_inputs_project(grad, input):\n",
    "    noise = torch.randn(grad.shape[0], device=DEVICE)\n",
    "    '''\n",
    "    inpt = torch.mean(input, dim = 0).view(-1)[None, :]\n",
    "\n",
    "    entry_from_batch = torch.randint(low=0, high=len(input), size=[])\n",
    "    inpt = input[entry_from_batch].view(-1)[None, :]\n",
    "    inpt /= inpt.norm()\n",
    "    '''\n",
    "    q,r = torch.qr(input.T)\n",
    "    entry_from_batch = torch.randint(low=0, high=len(input), size=[])\n",
    "    inpt = q.T[entry_from_batch].view(-1)[None, :]\n",
    "    expanded_noise = noise[:, None] * inpt\n",
    "    return expanded_noise * torch.sum(expanded_noise * grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212.353515625\n",
      "245.7314453125\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "0:0 [train] loss:2.467, [valid] acc:0.088, time: 2.3395713\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "0:20 [train] loss:2.405, [valid] acc:0.086, time: 0.0020577599999999975\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "0:40 [train] loss:2.369, [valid] acc:0.084, time: 0.0021218900000000264\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "0:60 [train] loss:2.453, [valid] acc:0.083, time: 0.0020190999999999625\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "0:80 [train] loss:2.390, [valid] acc:0.082, time: 0.002166215000000027\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "0:100 [train] loss:2.381, [valid] acc:0.084, time: 0.002528309999999934\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "0:120 [train] loss:2.375, [valid] acc:0.088, time: 0.0020442099999999908\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "0:140 [train] loss:2.323, [valid] acc:0.089, time: 0.0021437450000000523\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "0:160 [train] loss:2.302, [valid] acc:0.088, time: 0.0021179599999999075\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "0:180 [train] loss:2.335, [valid] acc:0.092, time: 0.00209388499999994\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n",
      "238.96142578125\n",
      "238.96142578125\n",
      "213.40478515625\n",
      "246.7822265625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-eaa03a1245af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mstats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_forward_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_forward_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-9a8d53de2942>\u001b[0m in \u001b[0;36mrun_experiment\u001b[1;34m(model, opt, train_dl, valid_dl, test_dl, use_forward_grad, num_forward_grad, max_epochs)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory_allocated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory_allocated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl, test_dl = get_mnist_dl(device=DEVICE)\n",
    "\n",
    "model = CNN_Net().to(DEVICE)\n",
    "\n",
    "for p in model.parameters():\n",
    "    g = p.view(-1)\n",
    "    v = torch.normal(mean = torch.full((1, len(g)), 0.), std = torch.full((1, len(g)), 0.1)).to(DEVICE)\n",
    "    p.data = v.view(p.shape)\n",
    "\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "stats = run_experiment(model, opt, train_dl, valid_dl, test_dl, use_forward_grad=False, num_forward_grad = 1, max_epochs=20)\n",
    "\n",
    "print_stats(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0747f93ff6db21b2db2bf35ad4858dd0825b9c21797c41b4cc32097944ab3f10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
