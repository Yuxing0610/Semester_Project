{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets #加载数据\n",
    "import torchvision.transforms as transforms #数据增强\n",
    "import matplotlib.pyplot as plt\n",
    "class Conv_2D():\n",
    "    def __init__(self, input_dim, output_dim, ksize=3,\n",
    "                 stride=1, padding=(0,0), dilataion=None):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.ksize = ksize\n",
    "        self.stride = stride\n",
    "        self.padding = padding #(1,2) 左边 和 上边 填充一列 右边和下边填充2列\n",
    "        self.dilatation = dilataion\n",
    "        self.output_h = None\n",
    "        self.output_w = None\n",
    "\n",
    "        self.patial_w = None\n",
    "        # 产生服从正态分布的多维随机随机矩阵作为初始卷积核\n",
    "        # OCHW\n",
    "        # self.conv_kernel = np.random.randn(self.output_dim, self.input_dim, self.kernelsize, self.kernelsize)  # O*I*k*k\n",
    "        self.grad = np.zeros((self.output_dim, self.ksize, self.ksize, self.input_dim), dtype=np.float64)\n",
    "        # 产生服从正态分布的多维随机随机矩阵作为初始卷积核\n",
    "        self.input = None\n",
    "        # OCh,w\n",
    "        self.weights = np.random.normal(scale=0.1,\n",
    "            size= (output_dim, input_dim, ksize, ksize))\n",
    "        self.weights.dtype =np.float64\n",
    "        self.bias = np.random.normal(scale=0.1,size = output_dim)\n",
    "        self.bias.dtype = np.float64\n",
    "\n",
    "        self.weights_grad = np.zeros(self.weights.shape)  # 回传到权重的梯度\n",
    "        self.bias_grad = np.zeros(self.bias.shape)  # 回传到bias的梯度\n",
    "        self.Jacobi = None  # 反传到输入的梯度\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "\n",
    "        :param input: (N,C,H,W)\n",
    "        :return:\n",
    "        '''\n",
    "        assert len(np.shape(input)) == 4\n",
    "        input = np.pad(input, ((0, 0), (0, 0), (self.padding[0], self.padding[1]),\n",
    "                               (self.padding[0], self.padding[1])), mode='constant', constant_values=0)\n",
    "        self.input = input\n",
    "\n",
    "        self.Jacobi = np.zeros(input.shape)\n",
    "        N, C, H, W = input.shape\n",
    "\n",
    "\n",
    "        # 输出大小\n",
    "        self.output_h = (H - self.ksize) / self.stride + 1\n",
    "        self.output_w = (W - self.ksize ) / self.stride + 1\n",
    "\n",
    "        # 检查是否是整数\n",
    "        assert self.output_h % 1 == 0\n",
    "        assert self.output_w % 1 == 0\n",
    "        self.output_h = int(self.output_h)\n",
    "        self.output_w = int(self.output_w)\n",
    "\n",
    "        imgcol = self.im2col(input, self.ksize, self.stride)  # (N*X,C*H*W)\n",
    "\n",
    "\n",
    "        output = np.dot(imgcol,\n",
    "                        self.weights.reshape(self.output_dim, -1).transpose(1, 0))  # (N*output_h*output_w,output_dim)\n",
    "\n",
    "\n",
    "        output += self.bias\n",
    "        output = output.reshape(N, self.output_w * self.output_h, self.output_dim). \\\n",
    "            transpose(0, 2, 1).reshape(N, int(self.output_dim), int(self.output_h), int(self.output_w))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, last_layer_delta,lr):\n",
    "        '''\n",
    "        计算传递到上一层的梯度\n",
    "        计算到weights 和bias 的梯度 并更新参数\n",
    "        :param last_layer_delta: 输出层的梯度 (N，output_dim,output_h,output_w)\n",
    "        :return:\n",
    "        '''\n",
    "        def judge_h(x):\n",
    "            if x % 1 == 0 and x <= self.output_h-1 and x >= 0:\n",
    "                return int(x)\n",
    "            else:\n",
    "                return -1\n",
    "        def judge_w(x):\n",
    "            if x % 1 == 0 and x <= self.output_w - 1 and x >= 0:\n",
    "                return int(x)\n",
    "            else:\n",
    "                return -1\n",
    "        # 根据推到出的公司 找出索引 与卷积权重相乘\n",
    "        for i in range(self.Jacobi.shape[2]):  # 遍历输入的高\n",
    "            for j in range(self.Jacobi.shape[3]):  # W\n",
    "                mask = np.zeros((self.input.shape[0], self.output_dim,\n",
    "                                 self.ksize, self.ksize))  # (N,O,k,k)\n",
    "                index_h = [(i - k) / self.stride for k in range(self.ksize)]\n",
    "                index_w = [(j - k) / self.stride for k in range(self.ksize)]\n",
    "                index_h_ = list(map(judge_h, index_h))\n",
    "                index_w_ = list(map(judge_w, index_w))\n",
    "\n",
    "                for m in range(self.ksize):\n",
    "                    for n in range(self.ksize):\n",
    "                        if index_h_[m] != -1 and index_w_[n] != -1:\n",
    "                            mask[:, :, m, n] = last_layer_delta[:, :, index_h_[m], index_w_[n]]  # (N,O,1,1)\n",
    "                        else:\n",
    "                            continue\n",
    "                mask = mask.reshape(self.input.shape[0], 1, self.output_dim, self.ksize, self.ksize)\n",
    "                self.Jacobi[:, :, i, j] = np.sum(mask * self.weights.transpose(1, 0, 2, 3), axis=(2, 3, 4))\n",
    "        # 去掉padding\n",
    "        self.Jacobi = self.Jacobi[:, :, self.padding[0]:self.input.shape[2]-self.padding[1],\n",
    "                      self.padding[0]:self.input.shape[3] - self.padding[1]]\n",
    "\n",
    "        # 计算 w\n",
    "        N,C,K,H,W = self.input.shape[0],self.input.shape[1],self.ksize**2,self.output_h,self.output_w\n",
    "        tmp = np.zeros((N,C,K,H,W))\n",
    "        for i in range(self.ksize):\n",
    "            for j in range(self.ksize):\n",
    "                #取出和对应位置相乘得数组\n",
    "                tmp[:,:,i*self.ksize+j,:,:] = self.input[:, :,i:self.output_h + i:self.stride, j:self.output_w + j:self.stride]\n",
    "       # print(tmp.shape)\n",
    "        tmp_new = np.sum(last_layer_delta.reshape(N,self.output_dim,1,1,H,W)*tmp.reshape(N,1,C,K,H,W),axis=(4,5)) #(N,O,C,K)\n",
    "       # print(tmp_new.shape)\n",
    "\n",
    "        self.weights_grad = np.sum(tmp_new.reshape(N,self.output_dim,C,self.ksize,self.ksize).transpose(1,2,0,3,4),axis=2) #(O,C,ksize,ksize)\n",
    "        # # 计算bias的梯度\n",
    "        tmp_bias = np.sum(last_layer_delta, axis=(2, 3))\n",
    "        self.bias_grad = np.sum(tmp_bias, axis=0)\n",
    "\n",
    "        ##\n",
    "        # for o in range(self.output_dim):  # 遍历每一个输出通道 每次处理一个卷积核\n",
    "        #     last_dz = last_layer_delta[:, o, :, :].reshape(last_layer_delta.shape[0], 1, self.output_h,\n",
    "        #                                                    self.output_w)  # (N,1,h,w)\n",
    "        #     #last_dz = np.repeat(last_dz, self.input_dim, axis=1)  # (N，inputdim,h,w)\n",
    "        #     for i in range(self.ksize):\n",
    "        #         for j in range(self.ksize):\n",
    "        #             tmp = np.sum(last_dz * self.input[:, :,\n",
    "        #                                    i:self.output_h + i:self.stride, j:self.output_w + j:self.stride],\n",
    "        #                          axis=(2, 3))\n",
    "        #             self.weights_grad[o, :, i, j] = np.sum(tmp, axis=0).reshape(-1, self.input_dim)\n",
    "        # # 计算bias的梯度\n",
    "        tmp_bias = np.sum(last_layer_delta, axis=(2, 3))\n",
    "        self.bias_grad = np.sum(tmp_bias, axis=0)\n",
    "        self.update(lr)\n",
    "        return self.Jacobi\n",
    "    def update(self,lr):\n",
    "        # print(self.weights_grad[0,0,0,:])\n",
    "        # print(self.bias_grad[:])\n",
    "\n",
    "        self.weights_grad[np.abs(self.weights_grad)<1e-10] = 0\n",
    "        self.bias_grad[np.abs(self.bias_grad) < 1e-10] = 0\n",
    "\n",
    "        self.weights_grad[self.weights_grad > 100] = 100\n",
    "        self.bias_grad[self.bias_grad>100] = 100\n",
    "\n",
    "        self.weights_grad[self.weights_grad < -100] = -100\n",
    "        self.bias_grad[self.bias_grad < -100] = -100\n",
    "\n",
    "\n",
    "        self.weights -= lr * self.weights_grad\n",
    "        self.bias -= lr * self.bias_grad\n",
    "\n",
    "\n",
    "    def im2col(self, image, ksize, stride):\n",
    "        '''\n",
    "        将输入图片矩阵化\n",
    "        N,C,H,W\n",
    "        :param image:\n",
    "        :param ksize:\n",
    "        :param stride:\n",
    "        :return:\n",
    "        '''\n",
    "        # image is a 4d tensor(N,C,H,W)\n",
    "        N, C, H, W = image.shape\n",
    "        image_col = []\n",
    "        for i in range(0, H - ksize + 1, stride):\n",
    "            for j in range(0, W - ksize + 1, stride):\n",
    "                col = image[:, :, i:i + ksize, j:j + ksize].reshape(N, -1)\n",
    "\n",
    "                image_col.append(col)\n",
    "        image_col = np.array(image_col)\n",
    "\n",
    "        return image_col.transpose(1, 0, 2).reshape(-1, C * ksize * ksize)  # (N*X,C*H*W) #swap_axis\n",
    "\n",
    "class max_pooling_2D():\n",
    "    def __init__(self,input_dim =3,stride = 2,ksize=2,padding=0):\n",
    "        '''\n",
    "        :param input_dim:\n",
    "        :param stride:\n",
    "        :param padding: padding数量\n",
    "        '''\n",
    "        self.input_dim = input_dim\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.stride = stride\n",
    "        self.ksize = ksize\n",
    "        self.padding = padding\n",
    "        self.record  = None #记录取元素的位置\n",
    "\n",
    "        self.Jacobi = None\n",
    "    def forward(self,input):\n",
    "        '''\n",
    "\n",
    "        :param input: (batchsize,c,h,w)\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        assert len(np.shape(input)) == 4\n",
    "\n",
    "        self.record = np.zeros(input.shape)\n",
    "        #padding\n",
    "        input = np.pad(input, ((0, 0), (0, 0), (self.padding, self.padding),\n",
    "                               (self.padding, self.padding)), mode='constant', constant_values=0)\n",
    "        self.input = input\n",
    "         #\n",
    "        input_N, input_C, input_h, input_w = input.shape[0], input.shape[1], \\\n",
    "                                                 input.shape[2], input.shape[3]\n",
    "        # padding 操作\n",
    "        output_h = int((input_h - self.ksize + 2*self.padding) / self.stride + 1) #padding 操作\n",
    "        output_w = int((input_w - self.ksize + 2 * self.padding) / self.stride + 1)\n",
    "\n",
    "        output = np.zeros(((int(input_N),int(input_C),int(output_h),int(output_w))))\n",
    "\n",
    "        for n in np.arange(input_N):\n",
    "            for c in np.arange(input_C):\n",
    "                for i in range(output_h):\n",
    "                    for j in range(output_w):\n",
    "                        #（batchsize,c,k,k）\n",
    "                        x_mask = input[n,c,i*self.stride:i*self.stride+self.ksize,\n",
    "                                           j*self.stride:j*self.stride+self.ksize]\n",
    "                        # print(x_mask)\n",
    "                        # print(np.max(x_mask))\n",
    "                        # print(output[n, c, i, j])\n",
    "                        output[n,c,i,j] = np.max(x_mask)\n",
    "\n",
    "        self.output = output\n",
    "        return  output\n",
    "\n",
    "    def backward(self,next_dz):\n",
    "        '''\n",
    "\n",
    "        :param next_dz: (N，C，H,W)\n",
    "        :return:\n",
    "        '''\n",
    "        self.Jacobi = np.zeros(self.input.shape)\n",
    "        N, C, H, W = self.input.shape\n",
    "        _, _, out_h, out_w = next_dz.shape\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                #print(self.input[:,:, i * self.stride:i * self.stride + self.ksize,j * self.stride:j * self.stride + self.ksize].shape)\n",
    "                # print(input[n, c, i * self.stride:i * self.stride + self.ksize,j * self.stride:j * self.stride + self.ksize].shape)\n",
    "                flat_idx = np.argmax(self.input[:,:,i*self.stride:i*self.stride+self.ksize,\n",
    "                                   j*self.stride:j*self.stride+self.ksize].reshape(N,C,self.ksize*self.ksize),axis=2)\n",
    "\n",
    "                h_idx = (i*self.stride +flat_idx//self.ksize).reshape(-1) #(N*C) 确定行位置\n",
    "                w_idx = (j*self.stride +flat_idx%self.ksize).reshape(-1) #确定列位置\n",
    "\n",
    "                for k in range(N*C):\n",
    "                    self.Jacobi[k//C,k%C,h_idx[k],w_idx[k]] = next_dz[k//C,k%C,i,j] #对应回原来位置\n",
    "\n",
    "                # self.Jacobifor k in range(N*C)\n",
    "                # self.Jacobi[, c_list, h_idx.reshape(-1),w_idx.reshape(-1)] = next_dz[:,:,i,j]\n",
    "        # 返回去掉padding的雅可比矩阵\n",
    "        return self.Jacobi[:,:,self.padding:H-self.padding,self.padding:W-self.padding]\n",
    "\n",
    "class Relu(): #ReLu激活层\n",
    "    def __init__(self):\n",
    "        self.Jacobi = None\n",
    "    def forward(self,input):\n",
    "        '''\n",
    "        :param input: (N,C,H,W)\n",
    "        :return:\n",
    "        '''\n",
    "        output = (np.abs(input)+input)/2\n",
    "        self.Jacobi = output.copy()\n",
    "        self.Jacobi[self.Jacobi>0] = 1\n",
    "        return output\n",
    "\n",
    "    def backward(self,next_dz):\n",
    "        '''\n",
    "        :param next_dz: 上一层的梯度\n",
    "        :return:\n",
    "        '''\n",
    "        self.Jacobi = self.Jacobi*next_dz\n",
    "\n",
    "        return self.Jacobi\n",
    "\n",
    "\n",
    "class softmax():\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "        self.input_delta = None #记录计算过程的雅可比矩阵\n",
    "        self.Jacobi = None #反传到输入的雅可比矩阵\n",
    "    def forward(self,input):\n",
    "        '''\n",
    "        :param input: (batchsize,n) np数组\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        batch_size = input.shape[0]\n",
    "        #n = input.shape[1]\n",
    "        self.Jacobi = np.zeros(input.shape)\n",
    "        self.input_delta = np.zeros(input.shape)\n",
    "\n",
    "        x = np.exp(input)\n",
    "        ##\n",
    "        y = np.sum(x,axis=1).reshape(batch_size,1)\n",
    "        output = x/y\n",
    "        self.output = output\n",
    "\n",
    "        # z = np.zeros((batch_size,n,n)) #(batchsize,n,n)\n",
    "        #\n",
    "        # z =  (1/y*y).repeat(n**2,axis=1).reshape(z.shape)\n",
    "        # z = np.repeat(x,n,axis=1).reshape(z.shape)*z\n",
    "        # z = -z*x.reshape(batch_size,n,1)\n",
    "        # for i in range(n):\n",
    "        #     # print(z[:,i,i],x[:,i]/y)\n",
    "        #     z[:,i,i] += x[:,i]/y.reshape(batch_size)\n",
    "        # self.input_delta = z #记录中间计算过程\n",
    "        return output\n",
    "\n",
    "    def backward(self,last_layer_delta):\n",
    "        '''\n",
    "        :param last_layer_delta: (N,n)\n",
    "        :return:\n",
    "        '''\n",
    "        for n in range(last_layer_delta.shape[1]): #遍历 n\n",
    "\n",
    "            tmp = -(self.output*self.output[:,n].reshape(-1,1))\n",
    "            tmp[:,n]+=self.output[:,n]\n",
    "            self.Jacobi[:,n] = np.sum(last_layer_delta*tmp,axis=1)\n",
    "\n",
    "\n",
    "\n",
    "        # batchsize = last_layer_delta.shape[0]\n",
    "        # n = last_layer_delta.shape[1]\n",
    "\n",
    "        return self.Jacobi\n",
    "\n",
    "\n",
    "class Linear():\n",
    "    def __init__(self,input_num,output_num):\n",
    "        self.input_num, self.output_num = input_num,output_num\n",
    "        self.weights = np.random.normal(scale=0.1,size = (self.output_num,self.input_num)) #构建矩阵 (output_num,intput_num)\n",
    "        self.bias = np.random.normal(scale=0.1,size = (1,self.output_num)) #bias (output_dim,1)\n",
    "        self.input_delta = None\n",
    "        self.weights_grad=np.zeros(self.weights.shape) #记录W 梯度\n",
    "        self.bias_grad = np.zeros(self.bias.shape) #记录bias 梯度\n",
    "        self.Jocobi = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        :param input: batchsize * input_num\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        output = np.dot(self.weights,input.transpose(1,0))\n",
    "        self.input_delta = input  #记录计算过程\n",
    "        return output.T + self.bias #(B，output_num)\n",
    "\n",
    "    def backward(self,last_layer_delta,lr):\n",
    "        '''\n",
    "\n",
    "        :param last_layer_delta:  (batchsize,output_num)\n",
    "        :param lr: 学习率\n",
    "        :return:\n",
    "        '''\n",
    "        #计算传到上一层的梯度\n",
    "        self.Jacobi = np.dot(self.weights.T, last_layer_delta.T).T  # (input_dim,batchsize)\n",
    "\n",
    "        self.bias_grad =  np.sum(last_layer_delta,axis = 0) #沿行方向求和 （1,n）\n",
    "        self.weights_grad = np.dot(last_layer_delta.T,self.input_delta) #(output_num,input_num)\n",
    "        # self.Jacobi = np.dot(self.w_matrix.T,last_layer_delta.T) # (input_dim,batchsize)\n",
    "        ##update\n",
    "        # self.bias_grad[np.abs(self.bias_grad) > 5] = 0.0001\n",
    "        # self.weight_grad[np.abs(self.weight_grad) > 5] = 0.0001\n",
    "        self.weights_grad[np.abs(self.weights_grad) < 1e-10] = 0\n",
    "        self.bias_grad[np.abs(self.bias_grad) < 1e-10] = 0\n",
    "        self.weights_grad[self.weights_grad > 100] = 100\n",
    "        self.bias_grad[self.bias_grad > 100] = 100\n",
    "        self.weights_grad[self.weights_grad < -100] = -100\n",
    "        self.bias_grad[self.bias_grad < -100] = -100\n",
    "\n",
    "        self.bias-= lr*self.bias_grad\n",
    "        self.weights-= lr*self.weights_grad\n",
    "\n",
    "        return self.Jacobi # (batchsize,n)\n",
    "\n",
    "class Normal():\n",
    "    def __init__(self):\n",
    "        self.Jacobi = None\n",
    "        self.mean = None\n",
    "        self.tad = None\n",
    "    def forward(self,input):\n",
    "        pass\n",
    "\n",
    "class sigmoid():\n",
    "    def __init__(self):\n",
    "        self.Jacobi = None\n",
    "        self.input_delta = None\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        :param input: (batchsize,input_num)\n",
    "        :return:\n",
    "        '''\n",
    "        x = np.exp(-input)\n",
    "        output = 1/(1+x)\n",
    "\n",
    "        self.input_delta = x/(1+x)**2\n",
    "        return output\n",
    "\n",
    "    def backward(self, last_layer_delta):\n",
    "        '''\n",
    "        需要先将last_layer_delta reshape成输入相同形状\n",
    "        :param last_layer_delta:\n",
    "        :return:\n",
    "        '''\n",
    "        self.Jacobi = self.input_delta*last_layer_delta\n",
    "        return  self.Jacobi\n",
    "\n",
    "\n",
    "class CNN_Nets():\n",
    "    def __init__(self,lr=0.0001,batchsize=10):\n",
    "        '''\n",
    "        28*28 的输入大小 训练一个minist分类网络\n",
    "        '''\n",
    "        self.lr = lr\n",
    "        self.bachsize = batchsize\n",
    "\n",
    "        self.conv1 = Conv_2D(input_dim=1,output_dim=26,ksize = 5,stride = 1, padding =(0,0)) #(24,24)\n",
    "        self.Relu_1 = Relu()\n",
    "        #self.maxpooling_1 = max_pooling_2D(input_dim=26,stride=2,ksize=2) #(12,12)\n",
    "\n",
    "        self.conv2 = Conv_2D(input_dim=26,output_dim=52 ,ksize = 3, stride = 1, padding= (0,0)) #(10,10)\n",
    "        self.Relu_2 = Relu()\n",
    "\n",
    "        self.conv3 = Conv_2D(input_dim=52, output_dim=10, ksize=1, stride=1, padding=(0, 0))  # (10,10) #降维\n",
    "        self.Relu_3 = Relu()\n",
    "        #self.maxpooling_3 = max_pooling_2D(input_dim=52,stride=2,ksize=2) #(5,5)\n",
    "\n",
    "        self.fc_1 = Linear(input_num=22*22*10,output_num=1000)\n",
    "        self.sigmoid_1 = sigmoid()\n",
    "        self.fc_2 = Linear(input_num=1000,output_num=10)\n",
    "        self.softmax = softmax()\n",
    "\n",
    "        self.CrossEntropy = CrossEntropy()\n",
    "\n",
    "        self.outut = None\n",
    "        self.loss = None\n",
    "        self.Jacobi = None\n",
    "\n",
    "    def forward(self,input,labels):\n",
    "        '''\n",
    "        :param input: (n,c,h,w)\n",
    "        :param labels: (batchsize,10) 的one_hot编码\n",
    "        :return:\n",
    "        '''\n",
    "        N,C,H,W = input.shape\n",
    "        #卷积层1\n",
    "        output = self.conv1.forward(input)\n",
    "        output = self.Relu_1.forward(output)\n",
    "\n",
    "        #output = self.maxpooling_1.forward(input=output)\n",
    "        #卷积层2\n",
    "\n",
    "        output = self.conv2.forward(output)\n",
    "        output = self.Relu_2.forward(output)\n",
    "\n",
    "        output = self.conv3.forward(output)\n",
    "        output = self.Relu_3.forward(output)\n",
    "\n",
    "        #output = self.maxpooling_3.forward(input=output)\n",
    "        #卷积层3\n",
    "\n",
    "        #第一个全连接层\n",
    "        output = np.reshape(output,(N,-1))\n",
    "        output = self.fc_1.forward(output)\n",
    "        output = self.sigmoid_1.forward(output)\n",
    "        #第二个全连接层\n",
    "        output= self.fc_2.forward(output)\n",
    "        output = self.softmax.forward(output) #(batchsize,10)\n",
    "        self.output = output\n",
    "        #计算交叉熵和反传梯度\n",
    "        self.loss = self.CrossEntropy.forward(output,labels) #交叉熵\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "         grad = self.CrossEntropy.Jacobi\n",
    "         grad = self.softmax.backward(grad)\n",
    "         grad = self.fc_2.backward(grad,lr =self.lr)\n",
    "         grad = self.sigmoid_1.backward(grad)\n",
    "         grad = self.fc_1.backward(grad,lr = self.lr)\n",
    "         grad = grad.reshape(self.bachsize,10,22,22) #重新恢复成图像\n",
    "\n",
    "         #grad = self.maxpooling_3.backward(grad)\n",
    "         grad = self.Relu_3.backward(grad)\n",
    "         grad = self.conv3.backward(grad,lr= self.lr)\n",
    "         grad = self.Relu_2.backward(grad)\n",
    "         grad = self.conv2.backward(grad,lr=self.lr)\n",
    "         #grad = self.maxpooling_1.backward(grad)\n",
    "         grad = self.Relu_1.backward(grad)\n",
    "         grad = self.conv1.backward(grad,lr=self.lr)\n",
    "\n",
    "         return grad\n",
    "\n",
    "class CrossEntropy():\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.Jacobi = None\n",
    "\n",
    "    def forward(self,input,labels):\n",
    "        bachsize = input.shape[0]\n",
    "        loss = np.sum(-(labels * np.log(input) + (1 - labels) * np.log(1 - input)) / bachsize)\n",
    "        self.loss = loss\n",
    "        self.Jacobi = -(labels / input - input * (1 - labels) / (1 - input)) / bachsize\n",
    "        return loss\n",
    "    def backwards(self):\n",
    "        return self.Jacobi\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "    pass\n",
    "    #卷积层梯度反传检查 没毛病\n",
    "    #conv_layer_grad_check()\n",
    "    #线性层梯度反传检查 没毛病\n",
    "    #Linear_grad_check()\n",
    "\n",
    "    #softmax sigmoid 检查 反向传播存在问题  1029 14:00 已解决\n",
    "    #softmax_grad_check()\n",
    "\n",
    "    # torch_fc = torch.nn.Linear(500,100)\n",
    "    # print(torch_fc.weight.shape)\n",
    "    #softmax_grad_check()\n",
    "    # Loss_grad_check()\n",
    "    #Loss_grad_check()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n",
      "epoch [0/0]  loss [4.1014198023905974]\n",
      "epoch [0/1]  loss [4.283676503826402]\n",
      "epoch [0/2]  loss [3.3736903549021857]\n",
      "epoch [0/3]  loss [3.354127463373911]\n",
      "epoch [0/4]  loss [3.3217924974329422]\n",
      "epoch [0/5]  loss [3.308854791434271]\n",
      "epoch [0/6]  loss [3.0876504625564687]\n",
      "epoch [0/7]  loss [3.7753452484603525]\n",
      "epoch [0/8]  loss [3.267147692239119]\n",
      "epoch [0/9]  loss [3.397591270279108]\n",
      "epoch [0/10]  loss [3.1613858553794687]\n",
      "epoch [0/11]  loss [3.0686869653919535]\n",
      "epoch [0/12]  loss [3.3100674943785062]\n",
      "epoch [0/13]  loss [2.968953871615481]\n",
      "epoch [0/14]  loss [3.3181840082785863]\n",
      "epoch [0/15]  loss [3.227108847812257]\n",
      "epoch [0/16]  loss [3.2638823124117233]\n",
      "epoch [0/17]  loss [3.1302704003535493]\n",
      "epoch [0/18]  loss [3.229630423284961]\n",
      "epoch [0/19]  loss [3.1310155905281647]\n",
      "epoch [0/20]  loss [3.1625478865602097]\n",
      "--------iter:10 num [61]  acc: [0.305]--------\n",
      "epoch [0/21]  loss [2.9384713215958893]\n",
      "epoch [0/22]  loss [3.009825793234111]\n",
      "epoch [0/23]  loss [3.344176189046636]\n",
      "epoch [0/24]  loss [3.216220934447821]\n",
      "epoch [0/25]  loss [3.015195424068775]\n",
      "epoch [0/26]  loss [2.99908874678284]\n",
      "epoch [0/27]  loss [3.0597627875864717]\n",
      "epoch [0/28]  loss [3.0387275256089037]\n",
      "epoch [0/29]  loss [3.1821341786992203]\n",
      "epoch [0/30]  loss [2.9893998356418354]\n",
      "epoch [0/31]  loss [2.9738333539161523]\n",
      "epoch [0/32]  loss [2.847442656646571]\n",
      "epoch [0/33]  loss [3.0276694108277415]\n",
      "epoch [0/34]  loss [2.8833428984985545]\n",
      "epoch [0/35]  loss [2.7639913356826304]\n",
      "epoch [0/36]  loss [2.857534189766495]\n",
      "epoch [0/37]  loss [3.0665901815636545]\n",
      "epoch [0/38]  loss [2.914545101969906]\n",
      "epoch [0/39]  loss [2.910575048527239]\n",
      "epoch [0/40]  loss [2.733069912980528]\n",
      "--------iter:10 num [67]  acc: [0.335]--------\n",
      "epoch [0/41]  loss [2.9814333233750503]\n",
      "epoch [0/42]  loss [2.884905261797908]\n",
      "epoch [0/43]  loss [2.888665667366588]\n",
      "epoch [0/44]  loss [3.0075442426289802]\n",
      "epoch [0/45]  loss [2.6456002912793766]\n",
      "epoch [0/46]  loss [3.041494487311957]\n",
      "epoch [0/47]  loss [2.75521825996147]\n",
      "epoch [0/48]  loss [2.8973631249556275]\n",
      "epoch [0/49]  loss [2.5675031470359126]\n",
      "epoch [0/50]  loss [2.967269393778416]\n",
      "epoch [0/51]  loss [2.8106525118833345]\n",
      "epoch [0/52]  loss [2.710060415013758]\n",
      "epoch [0/53]  loss [2.7334965206945583]\n",
      "epoch [0/54]  loss [2.5628966663729376]\n",
      "epoch [0/55]  loss [2.911759778135275]\n",
      "epoch [0/56]  loss [2.624600500297558]\n",
      "epoch [0/57]  loss [2.6428176397233347]\n",
      "epoch [0/58]  loss [2.773387278053706]\n",
      "epoch [0/59]  loss [2.741774528429514]\n",
      "epoch [0/60]  loss [2.490893317142901]\n",
      "--------iter:10 num [98]  acc: [0.49]--------\n",
      "epoch [0/61]  loss [2.1489567018215108]\n",
      "epoch [0/62]  loss [2.6057316181346866]\n",
      "epoch [0/63]  loss [2.774700536925719]\n",
      "epoch [0/64]  loss [2.8606401902331804]\n",
      "epoch [0/65]  loss [2.437529421089173]\n",
      "epoch [0/66]  loss [2.75257219993012]\n",
      "epoch [0/67]  loss [2.758739203038047]\n",
      "epoch [0/68]  loss [2.4139068057142925]\n",
      "epoch [0/69]  loss [2.4634513397350006]\n",
      "epoch [0/70]  loss [2.4847628054589452]\n",
      "epoch [0/71]  loss [2.401254014803203]\n",
      "epoch [0/72]  loss [2.537694105568143]\n",
      "epoch [0/73]  loss [2.3482851697909783]\n",
      "epoch [0/74]  loss [2.5504681836340604]\n",
      "epoch [0/75]  loss [2.440105636512106]\n",
      "epoch [0/76]  loss [2.4134171623051808]\n",
      "epoch [0/77]  loss [2.3265092471451334]\n",
      "epoch [0/78]  loss [2.263380206615225]\n",
      "epoch [0/79]  loss [2.3335469801748263]\n",
      "epoch [0/80]  loss [2.1759910732709384]\n",
      "--------iter:10 num [116]  acc: [0.58]--------\n",
      "epoch [0/81]  loss [2.0753807531580395]\n",
      "epoch [0/82]  loss [2.255531400006986]\n",
      "epoch [0/83]  loss [2.22623605493507]\n",
      "epoch [0/84]  loss [2.244658917855798]\n",
      "epoch [0/85]  loss [2.484637457473197]\n",
      "epoch [0/86]  loss [2.208049579533587]\n",
      "epoch [0/87]  loss [2.0367423843427774]\n",
      "epoch [0/88]  loss [2.4661873393478717]\n",
      "epoch [0/89]  loss [2.16928039357836]\n",
      "epoch [0/90]  loss [2.046421669905077]\n",
      "epoch [0/91]  loss [1.78698927101505]\n",
      "epoch [0/92]  loss [2.185031806440474]\n",
      "epoch [0/93]  loss [1.799852550122571]\n",
      "epoch [0/94]  loss [2.4770029012823427]\n",
      "epoch [0/95]  loss [1.8710601260929456]\n",
      "epoch [0/96]  loss [1.6478871056623032]\n",
      "epoch [0/97]  loss [1.8852342915159936]\n",
      "epoch [0/98]  loss [1.9550639868272675]\n",
      "epoch [0/99]  loss [2.093703743132352]\n",
      "epoch [0/100]  loss [2.2506929440985837]\n",
      "--------iter:10 num [134]  acc: [0.67]--------\n",
      "epoch [0/101]  loss [1.94024647040804]\n",
      "epoch [0/102]  loss [1.9680620990746214]\n",
      "epoch [0/103]  loss [1.987493285806524]\n",
      "epoch [0/104]  loss [1.9920023612679478]\n",
      "epoch [0/105]  loss [1.8258961042928528]\n",
      "epoch [0/106]  loss [1.8101693290069434]\n",
      "epoch [0/107]  loss [1.7307956535169822]\n",
      "epoch [0/108]  loss [1.831732965733788]\n",
      "epoch [0/109]  loss [1.771201209700339]\n",
      "epoch [0/110]  loss [2.222926878021993]\n",
      "epoch [0/111]  loss [1.8907596307711139]\n",
      "epoch [0/112]  loss [2.314905992820374]\n",
      "epoch [0/113]  loss [2.0024596843335853]\n",
      "epoch [0/114]  loss [1.8742168202760325]\n",
      "epoch [0/115]  loss [1.4932153223227604]\n",
      "epoch [0/116]  loss [1.8549700263209226]\n",
      "epoch [0/117]  loss [1.9588421795243296]\n",
      "epoch [0/118]  loss [1.7017664122768568]\n",
      "epoch [0/119]  loss [1.5125897432898943]\n",
      "epoch [0/120]  loss [1.9688430356073006]\n",
      "--------iter:10 num [141]  acc: [0.705]--------\n",
      "epoch [0/121]  loss [1.842471552589224]\n",
      "epoch [0/122]  loss [1.3577785392555204]\n",
      "epoch [0/123]  loss [1.7558251721973426]\n",
      "epoch [0/124]  loss [1.425785786905545]\n",
      "epoch [0/125]  loss [1.7101504236251228]\n",
      "epoch [0/126]  loss [1.6819362733715424]\n",
      "epoch [0/127]  loss [1.9762079307376654]\n",
      "epoch [0/128]  loss [1.3870425689258217]\n",
      "epoch [0/129]  loss [1.7555280913544886]\n",
      "epoch [0/130]  loss [1.8686426917331698]\n",
      "epoch [0/131]  loss [1.8529172869869317]\n",
      "epoch [0/132]  loss [1.7238393413804056]\n",
      "epoch [0/133]  loss [1.332004068121198]\n",
      "epoch [0/134]  loss [2.0949898453094313]\n",
      "epoch [0/135]  loss [1.1845971282783943]\n",
      "epoch [0/136]  loss [2.1252258680071585]\n",
      "epoch [0/137]  loss [1.3050446699061937]\n",
      "epoch [0/138]  loss [1.6147518366213525]\n",
      "epoch [0/139]  loss [1.3600201754073025]\n",
      "epoch [0/140]  loss [1.4791604266908402]\n",
      "--------iter:10 num [152]  acc: [0.76]--------\n",
      "epoch [0/141]  loss [1.2889973060958038]\n",
      "epoch [0/142]  loss [1.4250217272623154]\n",
      "epoch [0/143]  loss [1.4777640691339826]\n",
      "epoch [0/144]  loss [1.3271423132840798]\n",
      "epoch [0/145]  loss [1.3375248831145363]\n",
      "epoch [0/146]  loss [1.3929121454583986]\n",
      "epoch [0/147]  loss [1.0751661129703516]\n",
      "epoch [0/148]  loss [2.0344120068197027]\n",
      "epoch [0/149]  loss [1.1466778925849295]\n",
      "epoch [0/150]  loss [1.5449895807691565]\n",
      "epoch [0/151]  loss [1.6497248179625874]\n",
      "epoch [0/152]  loss [1.6364856359058015]\n",
      "epoch [0/153]  loss [1.3906745566037242]\n",
      "epoch [0/154]  loss [1.281451327897023]\n",
      "epoch [0/155]  loss [1.7206439117056298]\n",
      "epoch [0/156]  loss [1.2915763719004076]\n",
      "epoch [0/157]  loss [1.333390852304761]\n",
      "epoch [0/158]  loss [0.9988176827661495]\n",
      "epoch [0/159]  loss [1.3253783845243636]\n",
      "epoch [0/160]  loss [1.0714906745020691]\n",
      "--------iter:10 num [155]  acc: [0.775]--------\n",
      "epoch [0/161]  loss [1.101489134031846]\n",
      "epoch [0/162]  loss [1.9320494963099677]\n",
      "epoch [0/163]  loss [1.193040583533661]\n",
      "epoch [0/164]  loss [1.8541019698745027]\n",
      "epoch [0/165]  loss [1.0745340530542398]\n",
      "epoch [0/166]  loss [1.462142484016041]\n",
      "epoch [0/167]  loss [1.2734184905065171]\n",
      "epoch [0/168]  loss [0.7532369183077855]\n",
      "epoch [0/169]  loss [1.3270781059702554]\n",
      "epoch [0/170]  loss [1.175209234314776]\n",
      "epoch [0/171]  loss [1.6547801457356848]\n",
      "epoch [0/172]  loss [1.4393400999223414]\n",
      "epoch [0/173]  loss [1.2474640086045246]\n",
      "epoch [0/174]  loss [1.161663714292801]\n",
      "epoch [0/175]  loss [1.3270545564197611]\n",
      "epoch [0/176]  loss [1.2379819761849784]\n",
      "epoch [0/177]  loss [1.4500975932120401]\n",
      "epoch [0/178]  loss [1.3347895602068676]\n",
      "epoch [0/179]  loss [1.256266882695483]\n",
      "epoch [0/180]  loss [0.9276169869804735]\n",
      "--------iter:10 num [164]  acc: [0.82]--------\n",
      "epoch [0/181]  loss [1.525153811965008]\n",
      "epoch [0/182]  loss [0.9870716735795584]\n",
      "epoch [0/183]  loss [1.4026109351272433]\n",
      "epoch [0/184]  loss [1.5163858198217373]\n",
      "epoch [0/185]  loss [0.806491517275706]\n",
      "epoch [0/186]  loss [0.8703678129582902]\n",
      "epoch [0/187]  loss [1.0801302146814808]\n",
      "epoch [0/188]  loss [1.5702295133535356]\n",
      "epoch [0/189]  loss [1.1223252453444046]\n",
      "epoch [0/190]  loss [1.1689259663396252]\n",
      "epoch [0/191]  loss [0.897416510600227]\n",
      "epoch [0/192]  loss [1.55354235424061]\n",
      "epoch [0/193]  loss [1.1290588241912678]\n",
      "epoch [0/194]  loss [0.7350001266871482]\n",
      "epoch [0/195]  loss [1.1461609878603563]\n",
      "epoch [0/196]  loss [1.5620637811072622]\n",
      "epoch [0/197]  loss [1.1395403659818877]\n",
      "epoch [0/198]  loss [1.0935061621655753]\n",
      "epoch [0/199]  loss [1.202110160223432]\n",
      "epoch [0/200]  loss [1.2738430866396353]\n",
      "--------iter:10 num [172]  acc: [0.86]--------\n",
      "epoch [0/201]  loss [1.4755855539070808]\n",
      "epoch [0/202]  loss [1.3728102052741895]\n",
      "epoch [0/203]  loss [1.020838155258963]\n",
      "epoch [0/204]  loss [1.144397075635377]\n",
      "epoch [0/205]  loss [0.8163532915708382]\n",
      "epoch [0/206]  loss [1.4019084432739342]\n",
      "epoch [0/207]  loss [1.4226408625157945]\n",
      "epoch [0/208]  loss [0.926096616159956]\n",
      "epoch [0/209]  loss [0.8593151889789324]\n",
      "epoch [0/210]  loss [0.8396162828323079]\n",
      "epoch [0/211]  loss [1.0479902063450601]\n",
      "epoch [0/212]  loss [1.181480114233129]\n",
      "epoch [0/213]  loss [1.606788915984969]\n",
      "epoch [0/214]  loss [0.9046039898328345]\n",
      "epoch [0/215]  loss [1.457017459303893]\n",
      "epoch [0/216]  loss [1.0110455070371132]\n",
      "epoch [0/217]  loss [0.9106639691044891]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-0cf78b75d8f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"epoch [{}/{}]  loss [{}]\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[0mNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m20\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-0f39c42924fa>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    497\u001b[0m          \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m          \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRelu_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m          \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    500\u001b[0m          \u001b[1;31m#grad = self.maxpooling_1.backward(grad)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m          \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRelu_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-0f39c42924fa>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, last_layer_delta, lr)\u001b[0m\n\u001b[0;32m    120\u001b[0m                 \u001b[0mtmp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mksize\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_h\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_w\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m        \u001b[1;31m# print(tmp.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mtmp_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_layer_delta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#(N,O,C,K)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m        \u001b[1;31m# print(tmp_new.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.datasets as datasets #加载数据\n",
    "import torchvision.transforms as transforms #数据增强\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def Test(Net, dataloader):\n",
    "    num = 0\n",
    "    for i, (data, label) in enumerate(dataloader):\n",
    "        # print(data.shape,label.shape)\n",
    "        # print(data,label)\n",
    "        input = data.numpy()\n",
    "        label = label.numpy()\n",
    "        one_hot_labels = np.zeros((batch_size, 10))\n",
    "        one_hot_labels[[i for i in range(batch_size)], label] = 1\n",
    "        Net.forward(input, one_hot_labels)\n",
    "        predict = np.argmax(Net.output, axis=1)\n",
    "        num += np.sum(predict == label)\n",
    "        if i%20==0 and i!=0:\n",
    "            print('--------iter:{} num [{}]  acc: [{}]--------'.format(i, num, num / ((i+1)* input.shape[0])))\n",
    "    print('---------acc: [{}] ------'.format(num/10000))\n",
    "\n",
    "    return num/10000\n",
    "\n",
    "def validate(Net,dataloader,interval):\n",
    "\n",
    "    num = 0\n",
    "    for i, (data, label) in enumerate(dataloader):\n",
    "        # print(data.shape,label.shape)\n",
    "        # print(data,label)\n",
    "        input = data.numpy()\n",
    "        label = label.numpy()\n",
    "        one_hot_labels = np.zeros((batch_size, 10))\n",
    "        one_hot_labels[[i for i in range(batch_size)], label] = 1\n",
    "        Net.forward(input, one_hot_labels)\n",
    "        predict = np.argmax(Net.output,axis=1)\n",
    "        num+= np.sum(predict==label)\n",
    "        if i==interval-1:\n",
    "\n",
    "            print('--------iter:{} num [{}]  acc: [{}]--------'.format(i+1,num,num/(interval*input.shape[0])))\n",
    "            break\n",
    "    return  num/(interval*input.shape[0])\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # hyper parameter\n",
    "    input_size = 28 * 28  # image size of MNIST data\n",
    "    num_classes = 10\n",
    "    num_epochs = 10\n",
    "    batch_size = 20\n",
    "    lr = 1e-2\n",
    "    epoch_num = 100\n",
    "    train_dataset = datasets.MNIST(root='mnist/',  # 选择数据的根目录\n",
    "                                train=True,  # 选择训练集\n",
    "                                transform=transforms.ToTensor(),  # 转换成tensor变量\n",
    "                                download=True)  # 不从网络上download图片\n",
    "    test_dataset = datasets.MNIST(root='mnist/',  # 选择数据的根目录\n",
    "                               train=False,  # 选择训练集\n",
    "                               transform=transforms.ToTensor(),  # 转换成tensor变量\n",
    "                               download=True)  # 不从网络上download图片\n",
    "    # 加载数据\n",
    "    print(len(train_dataset))\n",
    "    print(len(test_dataset))\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=True)  # 将数据打乱\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle= True)\n",
    "    Net = CNN_Nets(lr , batch_size)\n",
    "\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        for i,(data,label) in enumerate(train_loader):\n",
    "            # print(data.shape,label.shape)\n",
    "            # print(data,label)\n",
    "            input = data.numpy()\n",
    "            label = label.numpy()\n",
    "            one_hot_labels = np.zeros((batch_size,10))\n",
    "            one_hot_labels[[i for i in range(batch_size)], label] = 1\n",
    "            Net.forward(input,one_hot_labels)\n",
    "            ##损失\n",
    "\n",
    "            print(\"epoch [{}/{}]  loss [{}]\".format(epoch,i,Net.loss))\n",
    "            Net.backward()\n",
    "\n",
    "            if i%20 == 0 and i!=0:\n",
    "                acc = validate(Net, test_loader,10)\n",
    "        if epoch%5==0 and epoch!=0:\n",
    "            Test(Net, test_loader)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0747f93ff6db21b2db2bf35ad4858dd0825b9c21797c41b4cc32097944ab3f10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
