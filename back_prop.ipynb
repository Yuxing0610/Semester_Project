{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "    print(\"using cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    print(\"using cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_to_device(dataset, device = None):\n",
    "    tensor_list_X, tensor_list_Y = [], []\n",
    "    for x, y in dataset:\n",
    "        tensor_list_X.append(x)\n",
    "        tensor_list_Y.append(y)\n",
    "    \n",
    "    X = torch.stack(tensor_list_X)\n",
    "    Y = torch.tensor(tensor_list_Y)\n",
    "    if device is not None:\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "    return torch.utils.data.TensorDataset(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_dl(batch_size_train = 256, batch_size_valid = 1024, device = None):\n",
    "    #transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5),(0.5))])\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "    data_train = MNIST('./datasets', train = True, download = True, transform = transform)\n",
    "    data_train = switch_to_device(data_train, device)\n",
    "    data_train, data_valid = torch.utils.data.random_split(data_train, [55000, 5000])\n",
    "\n",
    "    data_test = MNIST('./datasets', train = False, download = True, transform = transform)\n",
    "    data_test = switch_to_device(data_test, device)\n",
    "\n",
    "    train_dl = DataLoader(data_train, batch_size = batch_size_train, shuffle = True)\n",
    "    valid_dl = DataLoader(data_valid, batch_size = batch_size_valid, shuffle = False)\n",
    "    test_dl = DataLoader(data_test, batch_size = batch_size_valid, shuffle = False)\n",
    "\n",
    "    return train_dl, valid_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(stats):\n",
    "\n",
    "  fig, (ax1, ax2) = plt.subplots(1,2,figsize=(7,3), dpi=110)\n",
    "  ax1.grid()\n",
    "  ax2.grid()\n",
    "\n",
    "  ax1.set_title(\"ERM loss\")\n",
    "  ax2.set_title(\"Valid Acc\")\n",
    "  \n",
    "  ax1.set_xlabel(\"iterations\")\n",
    "  ax2.set_xlabel(\"iterations\")\n",
    "\n",
    "  itrs = [x[0] for x in stats['train-loss']]\n",
    "  loss = [x[1] for x in stats['train-loss']]\n",
    "  ax1.plot(itrs, loss)\n",
    "\n",
    "  itrs = [x[0] for x in stats['valid-acc']]\n",
    "  acc = [x[1] for x in stats['valid-acc']]\n",
    "  ax2.plot(itrs, acc)\n",
    "\n",
    "  ax1.set_ylim(0.0, 14.05)\n",
    "  ax2.set_ylim(0.0, 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_2D():\n",
    "    def __init__(self, input_dim, output_dim, k_size = 3, stride = 1, padding = 1):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.k_size = k_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        #The dimension of weight is (OIKK)\n",
    "        self.weight = torch.normal(mean = torch.full((self.output_dim, self.input_dim, self.k_size, self.k_size), 0.), std = torch.full((self.output_dim, self.input_dim, self.k_size, self.k_size), 0.1)).to(DEVICE)\n",
    "        #The dimension of bias is (O1)\n",
    "        self.bias = torch.normal(mean = torch.full([self.output_dim], 0.), std = torch.full([self.output_dim], 0.1)).to(DEVICE)\n",
    "        self.weights_grad = torch.zeros(self.weight.shape).to(DEVICE)\n",
    "        self.bias_grad = torch.zeros(self.bias.shape).to(DEVICE)\n",
    "\n",
    "        self.Jacobi = None\n",
    "        self.input = None\n",
    "        self.output_h = None\n",
    "        self.output_w = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        input : (N,I,H,W)\n",
    "        '''\n",
    "        m = nn.ZeroPad2d(self.padding)\n",
    "        input = m(input)\n",
    "        self.input = input\n",
    "        self.Jacobi = torch.zeros(input.shape)\n",
    "        N, C, H, W = input.shape\n",
    "        self.output_h = int((H - self.k_size) / self.stride + 1)\n",
    "        self.output_w = int((W - self.k_size) /self.stride + 1)\n",
    "        \n",
    "        unfold_input = nn.functional.unfold(input,(self.k_size, self.k_size))  #(N*(I*K*K)*-1)\n",
    "        output = unfold_input.transpose(1,2).matmul(self.weight.view(self.weight.shape[0], -1).t()).transpose(1,2) #(N*O*-1)\n",
    "        output = nn.functional.fold(output, (self.output_h, self.output_w), (1,1))\n",
    "        output = output + self.bias.view(1, -1, 1, 1)\n",
    "        return output\n",
    "    \n",
    "    '''\n",
    "    def backward(self, Next_Jacobi):\n",
    "        \n",
    "        #Next_Jacobi: (N, output_dim, output_h output_w)\n",
    "        \n",
    "        def judge_h(x):\n",
    "            if x % 1 == 0 and x < self.output_h and x > -1:\n",
    "                return int(x)\n",
    "            else:\n",
    "                return -1\n",
    "        \n",
    "        def judge_w(x):\n",
    "            if x % 1 == 0 and x < self.output_w and x > -1:\n",
    "                return int(x)\n",
    "            else:\n",
    "                return -1\n",
    "\n",
    "        #for the value at one position in input\n",
    "        for i in range(self.Jacobi.shape[2]):\n",
    "            for j in range(self.Jacobi.shape[3]):\n",
    "                mask = torch.zeros((self.input.shape[0], self.output_dim, self.k_size, self.k_size)).to(DEVICE)\n",
    "                #What positions in the output tensor can this vale effect\n",
    "                index_h = [(i - k) / self.stride for k in range(self.k_size)]\n",
    "                index_w = [(j - k) / self.stride for k in range(self.k_size)]\n",
    "                index_h_ = list(map(judge_h, index_h))\n",
    "                index_w_ = list(map(judge_w, index_w))\n",
    "\n",
    "                for m in range(self.k_size):\n",
    "                    for n in range(self.k_size):\n",
    "                        if index_h_[m] != -1 and index_w_[n] != -1:\n",
    "                            mask[:, :, m, n] = Next_Jacobi[:, :, index_h_[m], index_w_[n]] \n",
    "                        else:\n",
    "                            continue\n",
    "                \n",
    "                mask = mask.reshape(self.input.shape[0], 1, self.output_dim, self.k_size, self.k_size)\n",
    "                Jacobi_t = mask * self.weight.permute(1, 0, 2, 3) #(N, 1, O, K, K)*(I, O, K, K) = (N, I, O, K, K)\n",
    "                Jacobi_s_t = torch.sum(Jacobi_t, dim = (2, 3, 4))\n",
    "                self.Jacobi[:, :, i, j] = Jacobi_s_t\n",
    "        \n",
    "        #Get rid of padding\n",
    "        self.Jacobi = self.Jacobi[:, :, self.padding:self.input.shape[2]-self.padding, self.padding:self.input.shape[3]-self.padding].to(DEVICE)\n",
    "\n",
    "        N, C, K, H, W = self.input.shape[0], self.input.shape[1], self.k_size**2, self.output_h, self.output_w\n",
    "        tmp = torch.zeros((N,C,K,H,W)).to(DEVICE)\n",
    "        for i in range(self.k_size):\n",
    "            for j in range(self.k_size):\n",
    "                tmp[:, :, i*self.k_size + j, :, :] = self.input[:, :, i : self.output_h + i : self.stride, j : self.output_w + j : self.stride]\n",
    "\n",
    "        tmp_new = torch.sum(Next_Jacobi.reshape(N, self.output_dim, 1, 1, H, W)*tmp.reshape(N,1,C,K,H,W), dim = (4, 5))\n",
    "\n",
    "        self.weights_grad = torch.sum(tmp_new.reshape(N, self.output_dim, C, self.k_size, self.k_size).permute(1, 2, 0, 3, 4), dim = 2)\n",
    "\n",
    "        tmp_bias = torch.sum(Next_Jacobi, dim = (2,3))\n",
    "        self.bias_grad = torch.sum(tmp_bias, dim = 0)\n",
    "\n",
    "        return self.Jacobi\n",
    "    '''\n",
    "\n",
    "    def backward(self, Next_Jacobi):\n",
    "        #Next_Jacobi: (N, output_dim, output_h output_w)\n",
    "        #Only consider output_h = output_w\n",
    "        padding_num = self.stride*self.input.shape[2] + self.k_size - self.stride - Next_Jacobi.shape[2]\n",
    "        padding_num /= 2\n",
    "        m = nn.ZeroPad2d(int(padding_num))\n",
    "        padded_Next_Jacobi = m(Next_Jacobi)\n",
    "\n",
    "        rotate_weight = self.weight.transpose(0, 1)\n",
    "        rotate_weight = torch.rot90(rotate_weight, k = 2, dims = [2, 3])\n",
    "        unfold_next_Jacobi = nn.functional.unfold(padded_Next_Jacobi,(self.k_size, self.k_size))\n",
    "        self.Jacobi = unfold_next_Jacobi.transpose(1,2).matmul(rotate_weight.reshape(rotate_weight.shape[0], -1).t()).transpose(1,2) #(N*O*-1)\n",
    "        self.Jacobi = nn.functional.fold(self.Jacobi, (self.input.shape[2], self.input.shape[3]), (1,1))\n",
    "\n",
    "        tmp_bias = torch.sum(Next_Jacobi, dim = (2,3))\n",
    "        self.bias_grad = torch.sum(tmp_bias, dim = 0)\n",
    "        \n",
    "        self.input = self.input.transpose(0,1)\n",
    "        Next_Jacobi = Next_Jacobi.transpose(0,1)\n",
    "        unfold_input = nn.functional.unfold(self.input, (Next_Jacobi.shape[2], Next_Jacobi.shape[3]))\n",
    "        tmp_weights_grad = unfold_input.transpose(1, 2).matmul(Next_Jacobi.reshape(Next_Jacobi.shape[0], -1).t()).transpose(1,2)\n",
    "        tmp_weights_grad = nn.functional.fold(tmp_weights_grad, (self.k_size, self.k_size), (1,1))\n",
    "        self.weights_grad = tmp_weights_grad.transpose(0,1)\n",
    "\n",
    "        return self.Jacobi\n",
    "\n",
    "    \n",
    "    def update(self, lr):\n",
    "        self.weight -= lr*self.weights_grad\n",
    "        self.bias -= lr*self.bias_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class max_pooling_2D():\n",
    "    def __init__(self, k_size = 2, stride = 2, padding = 0):\n",
    "        self.k_size = k_size\n",
    "        self.stride = stride\n",
    "    \n",
    "    def forward(self, input):\n",
    "\n",
    "        out = input.reshape(input.shape[0], input.shape[1], input.shape[2]//self.k_size, self.k_size, input.shape[3]//self.k_size, self.k_size)\n",
    "        out = out.max(dim = 3)[0].max(dim = 4)[0]\n",
    "        self.index = out.repeat_interleave(self.k_size, dim = 2).repeat_interleave(self.k_size, dim = 3) == input\n",
    "        return out\n",
    "\n",
    "    def backward(self, Next_Jacobi):      \n",
    "        return Next_Jacobi.repeat_interleave(self.k_size, dim = 2).repeat_interleave(self.k_size, dim = 3) * self.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    def __init__(self):\n",
    "        self.Jacobi = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = (torch.abs(input) + input)/2.0\n",
    "        self.Jacobi = output.clone()\n",
    "        self.Jacobi[self.Jacobi>0] = 1\n",
    "        return output\n",
    "    \n",
    "    def backward(self, Next_Jacobi):\n",
    "        self.Jacobi = self.Jacobi*Next_Jacobi\n",
    "        return self.Jacobi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__ (self, input_num, output_num):\n",
    "        self.input_num, self.output_num = input_num, output_num\n",
    "        self.weights = torch.normal(mean = torch.full((self.output_num, self.input_num), 0.), std = torch.full((self.output_num, self.input_num), 0.1)).to(DEVICE)\n",
    "        self.bias = torch.normal(mean = torch.full((1, self.output_num), 0.), std = torch.full((1, self.output_num), 0.1)).to(DEVICE)\n",
    "        self.weights_grad = torch.zeros(self.weights.shape).to(DEVICE)\n",
    "        self.bias_grad = torch.zeros(self.bias.shape).to(DEVICE)\n",
    "\n",
    "        self.Jacobi = None\n",
    "        self.input = None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = torch.matmul(self.weights, input.transpose(1, 0))\n",
    "        self.input = input\n",
    "        return output.T + self.bias\n",
    "\n",
    "    def backward(self, Next_Jacobi):\n",
    "        self.Jacobi = torch.matmul(self.weights.T, Next_Jacobi.T).T\n",
    "\n",
    "        self.bias_grad = torch.sum(Next_Jacobi, dim = 0)\n",
    "        self.weights_grad = torch.matmul(Next_Jacobi.T, self.input)\n",
    "        return self.Jacobi\n",
    "        \n",
    "    def update(self, lr):\n",
    "        self.bias -= lr*self.bias_grad\n",
    "        self.weights -= lr*self.weights_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax_CrossEntropy():\n",
    "    def __init__(self):\n",
    "        self.Jacobi = None\n",
    "        self.loss = None\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        batch_size = input.shape[0]\n",
    "        self.Jacobi = torch.zeros(input.shape).to(DEVICE)\n",
    "        x = torch.exp(input)\n",
    "        y = torch.sum(x, dim = 1).reshape(batch_size, 1)\n",
    "        softmax_output = x/y\n",
    "        loss = torch.sum(-(labels*torch.log(softmax_output))) / batch_size\n",
    "        self.loss = loss\n",
    "        self.Jacobi = (softmax_output - labels)/batch_size\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self):\n",
    "        return self.Jacobi\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Nets():\n",
    "    def __init__(self):\n",
    "        self.conv1 = Conv_2D(input_dim=1, output_dim=26, k_size=5, stride=1, padding=0) #(N, 26, 24, 24)\n",
    "        self.Relu_1 = Relu()\n",
    "        self.maxpooling_1 = max_pooling_2D(k_size=2, stride=2) #(N, 26, 12, 12)\n",
    "\n",
    "        self.conv2 = Conv_2D(input_dim=26, output_dim=52, k_size=3, stride=1, padding=0) #(N, 52, 10, 10)\n",
    "        self.Relu_2 = Relu()\n",
    "\n",
    "        self.conv3 = Conv_2D(input_dim=52, output_dim=10, k_size=1, stride=1, padding=0) #(N, 10, 10, 10)\n",
    "        self.Relu_3 = Relu()\n",
    "        self.maxpooling_3 = max_pooling_2D(k_size=2, stride=2) #(N, 10, 5, 5)\n",
    "\n",
    "        self.fc_1 = Linear(input_num=5*5*10, output_num=1000)\n",
    "        self.Relu_4 = Relu()\n",
    "\n",
    "        self.fc_2 = Linear(input_num=1000, output_num=10)\n",
    "        self.softmax_CrossEntropy = Softmax_CrossEntropy()\n",
    "\n",
    "        self.output = None\n",
    "        self.loss = None\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        N, C, H, W = input.shape\n",
    "\n",
    "        output = self.conv1.forward(input)\n",
    "        output = self.Relu_1.forward(output)\n",
    "        output = self.maxpooling_1.forward(output)\n",
    "\n",
    "        output = self.conv2.forward(output)\n",
    "        output = self.Relu_2.forward(output)\n",
    "\n",
    "        output = self.conv3.forward(output)\n",
    "        output = self.Relu_3.forward(output)\n",
    "        output = self.maxpooling_3.forward(output)\n",
    "\n",
    "        output = torch.reshape(output, (N, -1))\n",
    "        output = self.fc_1.forward(output)\n",
    "        output = self.Relu_4.forward(output)\n",
    "\n",
    "        output = self.fc_2.forward(output)\n",
    "        self.output = output\n",
    "        loss = self.softmax_CrossEntropy.forward(output, labels)\n",
    "        self.loss = loss\n",
    "    \n",
    "    def backward(self):\n",
    "        grad = self.softmax_CrossEntropy.Jacobi\n",
    "        grad = self.fc_2.backward(grad)\n",
    "        grad = self.Relu_4.backward(grad)\n",
    "        grad = self.fc_1.backward(grad)\n",
    "        grad = grad.reshape(grad.shape[0], 10, 5, 5)\n",
    "\n",
    "        grad = self.maxpooling_3.backward(grad)\n",
    "        grad = self.Relu_3.backward(grad)\n",
    "        grad = self.conv3.backward(grad)\n",
    "        grad = self.Relu_2.backward(grad)\n",
    "        grad = self.conv2.backward(grad)\n",
    "        grad = self.maxpooling_1.backward(grad)\n",
    "        grad = self.Relu_1.backward(grad)\n",
    "        grad = self.conv1.backward(grad)\n",
    "\n",
    "    def update(self, lr):\n",
    "        self.conv1.update(lr)\n",
    "        self.conv2.update(lr)\n",
    "        self.conv3.update(lr)\n",
    "        self.fc_1.update(lr)\n",
    "        self.fc_2.update(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Nets():\n",
    "    def __init__(self, device = DEVICE):\n",
    "        self.device = device\n",
    "        self.fc_1 = Linear(input_num = 28*28, output_num = 1024)\n",
    "        self.sigmoid_1 = Relu()\n",
    "        self.fc_2 = Linear(input_num = 1024, output_num = 1024)\n",
    "        self.sigmoid_2 = Relu()\n",
    "        self.fc_3 = Linear(input_num = 1024, output_num = 10)\n",
    "        self.softmax_CrossEntropy = Softmax_CrossEntropy()\n",
    "        self.output = None\n",
    "        self.loss = None\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        input = torch.reshape(input, (input.shape[0], 28*28))\n",
    "        output = self.fc_1.forward(input)\n",
    "        output = self.sigmoid_1.forward(output)\n",
    "        output = self.fc_2.forward(output)\n",
    "        output = self.sigmoid_2.forward(output)\n",
    "        output = self.fc_3.forward(output)\n",
    "        self.output = output\n",
    "        loss = self.softmax_CrossEntropy.forward(output, labels)\n",
    "        self.loss = loss\n",
    "\n",
    "    def backward(self):\n",
    "        grad = self.softmax_CrossEntropy.Jacobi\n",
    "        grad = self.fc_3.backward(grad)\n",
    "        grad = self.sigmoid_2.backward(grad)\n",
    "        grad = self.fc_2.backward(grad)\n",
    "        grad = self.sigmoid_1.backward(grad)\n",
    "        grad = self.fc_1.backward(grad)\n",
    "    \n",
    "    def update(self, lr):\n",
    "        self.fc_1.update(lr)\n",
    "        self.fc_2.update(lr)\n",
    "        self.fc_3.update(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_acc(model, dl):\n",
    "  acc = []\n",
    "  i= 0\n",
    "  for X, y in dl:\n",
    "    one_hot_y = torch.zeros(X.shape[0], 10).to(DEVICE)\n",
    "    one_hot_y[[i for i in range(X.shape[0])], [k.item() for k in y]] = 1\n",
    "    model.forward(X, one_hot_y)\n",
    "    acc.append(torch.argmax(model.output, dim=1) == y)\n",
    "    i+=1\n",
    "    if i == 3:\n",
    "      break\n",
    "  acc = torch.cat(acc)\n",
    "  acc = torch.sum(acc)/len(acc)\n",
    "  return acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_experiment(model, train_dl, valid_dl, test_dl, max_epochs=20, lr = 1e-3):\n",
    "\n",
    "  itr = -1\n",
    "  stats = {'train-loss': [], 'valid-acc':[]}\n",
    "  time_list = []\n",
    "  memory_list = []\n",
    "  for epoch in range(max_epochs):\n",
    "    for X, y in train_dl:\n",
    "        itr += 1\n",
    "        one_hot_y = torch.zeros(X.shape[0], 10).to(DEVICE)\n",
    "        one_hot_y[[i for i in range(X.shape[0])], [k.item() for k in y]] = 1\n",
    "        start = time.time()\n",
    "        model.forward(X, one_hot_y)\n",
    "        memory_list.append(torch.cuda.memory_allocated()/1024/1024)\n",
    "        model.backward()\n",
    "        model.update(lr)\n",
    "        time_list.append(time.time()-start)\n",
    "        stats['train-loss'].append((itr, model.loss.item()))\n",
    "\n",
    "        if itr % 20 == 0:\n",
    "\n",
    "          valid_acc = get_acc(model, valid_dl)\n",
    "          stats['valid-acc'].append((itr, valid_acc))\n",
    "          s = f\"{epoch}:{itr} [train] loss:{model.loss.item():.3f}, [valid] acc:{valid_acc:.3f}, time: {np.sum(time_list)/20}, memory: {np.sum(memory_list)/len(memory_list)} \"\n",
    "          print(s)\n",
    "          time_list = []\n",
    "          memory_list = []\n",
    "\n",
    "  test_acc = get_acc(model, test_dl)\n",
    "  print(f\"[test] acc:{test_acc:.3f}\")\n",
    "  return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 20\n",
    "train_batch = 1024\n",
    "valid_batch = 256\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0 [train] loss:5.612, [valid] acc:0.132, time: 0.03099067211151123, memory: 243.9423828125 \n",
      "0:20 [train] loss:5.139, [valid] acc:0.133, time: 0.0016548514366149902, memory: 256.0361328125 \n",
      "0:40 [train] loss:4.799, [valid] acc:0.135, time: 0.0016459584236145019, memory: 255.5205078125 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-49f3d4f50364>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP_Nets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mstats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-cc2c257ad76d>\u001b[0m in \u001b[0;36mrun_experiment\u001b[1;34m(model, train_dl, valid_dl, test_dl, max_epochs, lr)\u001b[0m\n\u001b[0;32m      7\u001b[0m   \u001b[0mmemory_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mitr\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mone_hot_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'numpy'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'string_'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl, test_dl = get_mnist_dl(batch_size_train=train_batch, batch_size_valid=valid_batch, device=DEVICE)\n",
    "\n",
    "model = MLP_Nets()\n",
    "\n",
    "stats = run_experiment(model, train_dl, valid_dl, test_dl, max_epochs=max_epochs, lr = lr)\n",
    "\n",
    "print_stats(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0747f93ff6db21b2db2bf35ad4858dd0825b9c21797c41b4cc32097944ab3f10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
